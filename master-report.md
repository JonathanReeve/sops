---
title: A Safer Online Public Square
authors: Jonathan Reeve, Colin Muller
---

# Problems

## Introdution 

Perhaps the most challenging step in developing social or technological tools for promoting a ‘safer online public square’ is defining what sort of speech constitutes a threat to the civility and safety of members of online communities.

Where do we draw the line between off-color and offensive content and content that inflicts harm or hate upon an individual or group? Additionally, how can a third party moderator (or algorithm) gauge the context of the speech as well as the the subjective perception of the speech by both speaker and target of the speech? 

Some critics of attempts to filter out harmful speech online claim that it violates first amendment free speech principles. These arguments vary, but generally point to the fear that content moderation may silence dissenting voices or unpopular opinions. On the other hand, defenders of content moderation assert that  social media platforms have the right to remove content at their own discretion as they are not government agents who are held to first amendment standards. First amendment legal scholars have split the debate of free speech and content intermediaries between [the right to speak and the right to hear/be heard](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1205674) , with some arguing that the mere right to speak is insufficient if media outlets are able to suppress certain speech on all platforms. Conversely, an individual's abilit to rapidly draft and publish a hateful tweet should require the target of the speech to view the harmful post.

Other concerns about content moderation include threats to anonymity and privacy, no platforming, [silencing marginalized voices](https://www.propublica.org/article/facebook-hate-speech-censorship-internal-documents-algorithms), [amplifying hate speech by calling it out](http://archives.cjr.org/minority_reports/reprint_reporting_and_race.php), and fixing virtual identites to legal identities through ['real name' policies](https://www.theguardian.com/world/2017/jun/29/facebook-real-name-trans-drag-queen-dottie-lux). 

There are many categories of speech that fall under the umbrella of harmful speech (The main categories are harassment, bullying, hate speech, and dangerous speech), but the boundaries between these different categories are neither rigid nor clear. The ambiguity posed by these various categories has led some to abandon formal definitions and seek different approaches to classifying harmful speech (see section below). Beyond this issue of classifying speech, there remains the problem of what one subjectively perceives as harmful speech; one [study](https://pdfs.semanticscholar.org/db55/11e90b2f4d650067ebf934294617eff81eca.pdf) found only 33% overall agreement between students of different races asked to assess the degree to which tweets were racially offensive.

### Statement of purpose of our preliminary report (to discuss at meeting)

## Taxonomies

There is significant corpus of writing which attempts to formulate a definition and taxonomy of harmful speech online. These classifications will often vary based on the purpose of the definition which varies from academic research, legal recommendations, or advocacy. These different purposes result in varying breadth and scope of definitions, and disagreement over definitions is one of the main challenges in compiling data from different sources on the frequency of harmful speech online (as the specific behavior being monitored in different studies varies widely). Despite these challenges, it is useful to outline the different categories of harmful online behavior, as these different types manifest themselves differently and will require different approaches for intervention.

 - Hate speech
 
Andrew Sellars conducted an [overview](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2882244) of various attempts to define hate speech by academics, legal experts, and online platforms. In lieu of defining hate speech he identifies 8 emerging themes and common traits that are used in defining hate speech:

1. Targeting of a group, or individuals as a member of a group
2. Content in the message that expresses hatred
3. The speech causes a harm
4. The speaker intends harm or bad activity
5. The speech incites bad actions beyond the speech itself
6. The speech is either public or directed at a member of a the group
7. The context makes violent response possible
8. The speech has no redeeming purpose

Sellars points out that there are two divergent theoretical fields that deal with hate speech: free speech theory and critical race theory, and there is little overlap between the two. 

Academic definitions often vary based on whether the goal is to apply the indention to legal sanctions or to simply understand the social phenomenon of hate speech. Some definitions focus more on the content of the speech, whereas others place more emphasis on intent of speech, and a third approach ignores the intention of the speaker and instead focuses on the outcome and effects of speech. Alice Marwick and Ross Miller have synthesized these varying approaches, and proposed the following three general elements used to define hate speech: 

1. Content-based elementary
2. Intent-based elementary
3. Harms-based element

Perhaps the clearest definition of hate speech is Susan Benesch’s: 
   - “An expression that denigrates or stigmatizes a person or people based on their membership of a group that is usually but not always immutable, such as an ethnic or religious group. Sometimes other groups, defined by disability or sexual orientation, for example, are included.” Source: Benesch, Susan. Defining and diminishing hate speech. 2014. 

The important difference between hate speech and other forms of harmful behavior online, is that hate speech is targeted at a group of people; even when it takes the form of hate speech against an individual it is targeting them for their belonging to a specific group
 

 - Bullying:
 
Definitions of cyberbullying largely rely on definitions of offline bullying which are characterized by 3 factors: 1) psychological torment 2) repeated behavior 3) carried out with intent. Social Psychologist Robert Tokunaga defines cyberbullying as “any behavior performed through electronic or digital media by individuals or groups that repeatedly communicates hostile or aggressive messages intended to inflict harm or discomfort on others.” While in traditional settings the factor of repeated behavior is more clear cut, there is some disagreement over what is considered repetition online, as a single post can be shared or otherwise interacted with repeatedly.  

While the definition of offline and online bullying are nearly identical, Tokunaga and others have identified several differences in the nature of offline and online bullying. Studies have found that individuals who don’t engage in traditional bullying are more likely to do so online since there is a lower threat level of being caught and reprimanded. While schools have clear norm-enforcing agents, there are no clear authority figures online who will regulate behavior. Cyberbullying is therefore considered as a more “opportunistic offense.” Additionally online bullying is less likely to occur at home, and bullies online are able to act anonymously and bully others they don’t know in ‘real life.’ However, most cyberbullying is done against people the bully knows.

Findings from research on traditional bullying can also be informative for future studies on cyberbullying and approaches to intervention. One finding is that kids often don’t identify with the term “bullying” often considering bullying acts to be “drama” or “beef.” As a result in surveys that asked about cyberbullying, between 20-40% of youth reported being victimized, but in surveys that ask about “mean things” online, 70% of youth reported such experiences. This is important to bear in mind when collecting survey data, but it also highlights another feature of bullying which is its cyclical nature. Teens often refer to bullying as drama which blurs the line between serious and nonserious conflict. This is because there is not always a clear distinction between categories of bully, victim, and bystander. In many surveys half of young respondents report being involved in both bullying and victimizaiton. This is referred to as “reactive” bullying, where a target of  bullying retaliates with similar bullying behavior. It is important to bear the cyclical nature of bullying in mind when crafting intervention models. 



Tokunaga, Robert S. "Following you home from school: A critical review and synthesis of research on cyberbullying victimization." Computers in human behavior 26.3 (2010): 278.


 - Harassment and stalking
 
 Online harassment is a term used to describe a variety of online behaviors that target an individual or group with the intention of harming them, getting them to remove content, or discouraging from expressing themselves online. Behaviors that constitute online harassment include “threats, continued hateful messages, doxxing, DDoS attacks, swatting, defamation, and more.” According to HeartMob “what separates online harassment from healthy discourse is the focus on harm: including publishing personal information, sending threats with the intention to scare or harm, using discriminatory language against an individual, and even directly promoting harm against a person or organization." 
Source: [HeartMob](https://iheartmob.org/pages/faqs#online-harassment)
 - Dangerous speech
   
   Dangerous speech is a form of online abuse that has high stakes. Susan Benesch characterizes it as ““speech that can inspire or catalyze intergroup violence.” The speaker of dangerous speech often hold positions of power with influence over a large base. The audience dangerous speech is addressed to often hold “grievances and fear that the speaker can cultivate,” and the speech plays on this in order to call on that group to act violently. Dangerous speech is most likely to occur in social or historical contexts that are already marked by or susceptible to violence, such as situations of civil war. 
   
   Source: [Counterspeech on Twitter: A Field Study](https://dangerousspeech.org/counterspeech-on-twitter-a-field-study/) & http://www.worldpolicy.org/sites/default/files/Dangerous%20Speech%20Guidelines%20Benesch%20January%202012.pdf 

 - Abuse of journalists
   - "Gamergate"
 - Related language patterns that, while not strictly abusive language, could be useful to its detection
   - Misinformation (e.g. Russian fake news)
   - Deceptive opinion spam (e.g. fake Amazon product reviews) 
   
### Alternative approaches to classifying harfmul speech:
#### Implicit v. Explicit & Generalized v. Directed Abusive Language 

Besides the approach of defining and taxonomizing different forms of hate speech, some have formulated other vectors for recognizing harmful speech online. One particularly useful approach is that developed by Waseem et al. 
Rather than attempting to define various terms such as hate speech, cyberbullying, and cyber harassment, they propose two primary vectors for typologizing abusive language—Implicit vs. Explicit and Generalized vs. Directed abusive language. To classify abusive speech they recommend asking the following questions:
Is the language directed towards a specific individual or entity or is it directed towards a generalized group? 
Is the abusive content explicit or implicit?
	Typologizing based on these two factors is useful as there can be a lot of overlap and ambiguity when relying on stricter definitions. However, one shortcoming of this typology is that by making the distinction between directed/generalized attacks, the research may downplay the fact that hate speech, even when verbally directed at an individual (ex: calling someone a racial slur) is an offense against an entire sub-group of people.

   - Source: [Waseem, Zeerak, et al. "Understanding Abuse: A Typology of Abusive Language Detection Subtasks." arXiv preprint arXiv:1705.09899 (2017).](http://www.aclweb.org/anthology/W17-3012)

#### Justice Stewart's rule: “I know it when I see it”
Supreme Court Justice Stewart famously asserted “I know it when I see it” when referring to identifying obscenity. It seems to be the consensus that this approach is not applicable to identifying hate speech due to the variety of forms of speech and contexts which one could identify as hate speech.

There are instances where specific epithets or insults are used and an outsider or scholar may see hate speech but the speaker/recipient do not. Henry Louis Gates Jr. for this reason asserted that we should not “spend more time worrying about speech codes than coded speech.” Since a lot of hate speech can be coded or masked by symbols.

The discussion of Stewart’s “I know it when I see it” points to a central difficulty in defining hate speech since it requires assessing the subjectivity and intention of both the perpetrator and the victim. However, only some definitions include the component of intention on the part of the perpetrator, and definitions also vary on how they define harm to the victim.

#### Brandenburg test

The 1969 supreme court case Brandenburg vs. Ohio narrowed the scope of unprotected speech under the first amendment. The court found that speech advocating illegal action is only prohibited when it is “directed to inciting or producing imminent lawless action and is likely to incite or produce such action.” Prior to this ruling the wording was applicable to vaguer, more generalized advocacy of illegal action. 
The case revolved around whether or not a KKK leader saying “it's possible that there might have to be some revengeance [sic] taken” in a speech full of racist epithets constituted speech that intended to advocate violent illegal action. The Supreme court ended up deciding that the statement was protected by the first amendment since it was abstract advocacy of violence or illegal action. The court set higher standards for prohibiting speech that advocates for illegal action, requiring that it be likely that the illegal action actually occur in the near future.  
What emerged from this case is the “Brandenburg test” which requires three elements for speech to be considered unprotected by the first amendment: intent, imminence, and likelihood. The court established that speech may be prohibited if it is: 
1. “directed to inciting or producing imminent lawless action”
2. “likely to incite or produce such action” 
source: https://www.law.cornell.edu/wex/brandenburg_test 

#### Elons v US

In 2015 the supreme court ruled on a case about Anthony Elonis who had posted rap lyrics on his facebook that violently threatened his ex-wife in graphic detail. In lower courts Elonis had been convicted of threatening  a person over interstate lines, but the supreme court reversed this ruling.  Their decision revolved around whether or not the standard of a “reasonable person” feeling threatened by the post was sufficient evidence to sentence Elonis. The supreme court found that there was also the necessary element of a “guilty mind” or “mens rea” on the part of the speaker which they said was absent because Elonis had consistently held that he posted the threatening lyrics for therapeutic purposes.
While this was the first time the Supreme Court heard a case related to speech on social media, the court largely avoided discussing first amendment related questions in the majority opinion. The judgment was only narrowly about whether or not mens rea was a requisite component of threatening a person.  
In Samuel Alito’s concurrence there is more discussion of First Amendment on social media. Alito distinguishes between rap lyrics and social media posts saying: "lyrics in songs that are performed for an audience or sold in recorded form are unlikely to be interpreted as a real threat to a real person." Whereas "Statements on social media that are pointedly directed at their victims, by contrast, are much more likely to be taken seriously."

#### What is left out of definitions

There are two forms of harmful speech that are often left out of discussions of harmful speech. The first is self-directed harmful speech and the second is harmful speech expressed privately between a group of like-minded people where the target of the harmful speech is not a party to the discussion. Neither is easily categorized with other forms of harmful speech and they therefore need separate attention.

- Harmful speech directed at one's self
  - this includes online posts that depict eating disorders or suggest self-harm. It is infrequently included in discussion of harmful online behavior, but it is a serious issue that needs to be dealt with in a different way than outward oriented harmful speech. 
- Harmful speech between a group that does not include the target of the speech
  - An example of this could be a private message channel where racists exchange racist messages about a group of people who are not a part of the message channel. Upon first consideration this speech may seem more like thought than it does speech in the public realm, but such conversation can have real life consequences on the well being of those discussed such as in private [message forums where doxxing information is collected](https://theintercept.com/2017/09/06/how-right-wing-extremists-stalk-dox-and-harass-their-enemies/) 

## Statistics 

 - General stats related to abusive langauge
 - Cyberbulling stats
 - Hate speech stats

[A 2014 Pew Research Center survey](http://www.pewinternet.org/2014/10/22/online-harassment/) found that 73% of adult Internet users have witnessed harassment online, and 40% have experienced it personally [@duggan_online_2014]. [A later 2016 report by the Data and Society Research Institute](https://www.datasociety.net/pubs/oh/Online_Harassment_2016.pdf) claims that, of American Internet users, 72% have witnessed harassment or abuse, and almost half (47%) have personally experienced it [@lenhart2016online]. 

The types of harassment include calling of names (reported by 60% of witnesses)  purposeful embarrassment (53%), physical threats (25%), sexual harassment (19%), and stalking (18%) [@duggan_online_2014]. Of those who had personally experienced harassment, 8% had been physically threatened or stalked, and 6% had been sexually harassed. These typically took place on social media platforms, although also in comments sections or in multiplayer games. 

The Pew report finds that young women, aged 18-24, are disproportionately targeted in all categories except for those of purposeful embarrassment and the calling of offensive names. [A 2013 report by the WHOA organization](http://www.haltabuse.org/resources/stats/index.shtml) (Working to Halt Online Abuse) echoes this finding. In their analysis of 4,043 self-reported cases of abuse in American from 2000-2013, they find that 70% of victims were female, with a 42% majority between the ages of 18 and 30. The abusers, they find, are more likely to be men (47%) than women (30%). LGB Internet users, as well, are more likely to experience harassment [@lenhart2016online 37]. 

## Potential Causes 

While theory building on the underlying causes of harmful speech online is generally underdeveloped and often not rigorously proven with empirical research, ([Tokunaga 2010](http://www.sciencedirect.com/science/article/pii/S074756320900185X)), existing social-psychological theories helps us better determine effective forms of intervention. This vein of research is particularly useful in emphasizing the impact that an individual’s harmful speech can have on their social group, since “cyberbystanders” witnessing of abusive language online impacts their  understanding of acceptable online norms. This research then reminds us that when intervening in the name of a safer online public sphere, it is not only the speaker and recipient that we must pay attention to, but also those bystanders and digital onlookers who may also happen to witness the encounter. 

### Anonymity: online disinhibition, deindividuation, and depersonalization

- Suler's theory of the [online disinhibtion effect](db19.linccweb.org/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=13621589&site=ehost-live)
  - Suler’s theory of the “online disinhibition effect” argues that online users compartmentalize their “online self” and “real life self” and the normal cognitive processes that guide their “real life” behavior are suspended when they are online. Online disinhibition is  a result of anonymity, asynchronous communicaiton, and empathy deficit.
- [Barlett and Gentile Model](http://www.apa.org/pubs/journals/features/ppm-ppm0000055.pdf)
  - Similar to Suler’s theory, Barlett and Gentile Model argues that attitudes towards cyberbullying mediate the relationship between anonymity and likely cyberbullying behavior since users who realize their anonymity online will dissociate their online actions from their “real” self which will contribute to positive cyberbullying attitude and increased cyberbullying. The model hypothesizes that perceived anonymity gives potential cyberbullies a sense of impunity online as well as empowerment since they can attack individuals they would not be able to attack offline either because of differences in physical strength or because they do not actually know or live near that person offline. Perceived anonymity leads cyberbullies to distance themselves from their own actions and gain a more positive perception of cyberbullying thus creating a feedback loop that encourages further cyberbullying.
  - Barlett and Gentile propose intervening against cyberbullying by informing “Internet users that they are not anonymous and show them evidence of IP address tracking and how History folders operate, then perhaps cyberbullying will decrease”  (178). However this form of intervention would not be effective in cases where users are actually able to be anonymous online (such as by using a mix network architecture).
  - A limitaiton to theories that argue that anonymity contributes to harfmul online behavior (such as those by Suler, Barlett, and Gentile) is the ambiguous definition of anonymity that doesn’t distinguish between pseudonymity, perceived anonymity, or partial anonymity.
- deindividuation V. Depersonalization 
  - Suler and Barlett and Gentile's theories suggest that a user online undergoes a sort of deindividuation, where they experience a loss of their sense of self. Other researchers have pushed back against the idea that online activity leads to deinidivuation and instead have put forward the notion of depersonalization as a better explanation. Depersonalization is defined as a “process through which individuals perceive that their certain group identity is more salient than other identities in a particular context, termed as ‘the emergence of group in the self’ (Huang 399). The principle difference is that deindividuation implies a loss of rationality that leads to necessarily harmful results whereas depersonalization is stripped of some of the value judgments and provides an explanation for both positive and negative group behavior online. Huang et al in their study ["The effect of anonymity on conformity to group norms in online contexts: a meta-analysis”](http://ijoc.org/index.php/ijoc/article/view/4037) put forward an argument for depersonalization online as they demonstrate that perceived anonymity results in group conformity online. 
  -  By understanding how anonymity online can lead to greater group conformity, we can see how this dynamic can be used to combat (as well as contribute to) harmful speech online. If an individual is part of an online community where harmful speech is perceived as unacceptable and uncommon, then they are less likely themselves to engage in harfmul speech.
    - A clear example of depersonalizaiton or ‘mob mentality’ being mobilized to combat harfmul speech online is the website [HeartMob](https://iheartmob.org/about) which allows for online bystander intervention and provides an immediate support group for people who have faced online harassment. 
 - depersonalization: prioritizing one's identity with the group over one's individualized identity

### Mimicry Effect
 - This theory holds that boosting positive content rather than deleting harmful content will foster an environment where other members of the website will contribute positive content
 
### Backfire effect
 - Political psychologists Nyhan and Reifler have researched the “backfire effect” where attempts at correcting misperceptions or misinformed beliefs results in firmer beliefs in the misperception or misinformation
   - “individuals who receive unwelcome information may not simply resist challenges to their views. Instead they may come to support their original opinion even more strongly”  Nyhan, Brendan, and Jason Reifler. "When corrections fail: The persistence of political misperceptions." Political Behavior 32.2 (2010): 307.
   - It is important to consider the backfire effect in looking at effective forms of intervention/counterspeech, since often presenting facts or engaging in logical/reasoned debate is not the most effective strategy
   - A similar psychological phenomenon is “motivated reasoning” where people make a strong effort to support the conclusions they seek despite being exposed to contradictory facts
   
### Temporal clustering of hate speech
  - prejudicial crimes are strongly influenced in the short term due to publicized events such as murders committed by a minority, this amplification in hate speech usually lasts about 2 weeks.  
  - Hashtags can be a good indicator of temporal clustering: ex: #killallmuslims #ferguson #charliehebdo #brussels #banislam #baltimore #mizzou
  - Source: King, R.D., and G.M. Sutton. 2013. “High Times for Hate Crime: Explaining the Temporal Clustering of Hate Motivated Offending.” Criminology 51 (4): 871–94

# Existing Approaches to Intervention
## Organizations 
### Advocacy Groups (Colin and Jonathan)

 - Working to Halt Online Abuse
 - No Hate Speech Movement
 - Etc...

A number of organizations exist to study and combat harassment, hate speech, and related phenomena. The [Women's Media Center Speech Project](http://wmcspeechproject.com/) studies and reports on "the underrepresentation and misrepresentation of women in the media," including online harassment of women. 
- [Crash Override Network](http://www.crashoverridenetwork.com/)
   - Crash Override Network provides a hotline for crisis support and assistance to the targets of online harassment. They also engage in community outreach and education. It was founded by Zoe Quinn, a game developer who was one of the first targets of GamerGate. They also work with tech companies and government officials to better protect against online harassment.   
- [Trollbusters](www.troll-busters.com)
   - TrollBusters is a global, collaborative campaign project dedicated to helping journalists that are under attack. It calls itself "Online Pest Control for Women Writers.” Someone being harassed can send a message to TrollBusters who will respond by flooding their feed with positive, supportive messages to serve as a counternarrative to the harassment. They also provide resources that give some legal and psychological guidance to women authors.They are also developing software to identify and ‘out’ trolls. In 2015 it won a top prize at the International Women’s Media Foundation hackathon.
   - learn more [here](https://yoursosteam.wordpress.com/) and [here](https://www.cjr.org/analysis/the_invaluable_service_of_trollbuster.php)
- [Take Back the Tech](https://www.takebackthetech.net/about) 
    - “a global, collaborative campaign project that highlights the problem of tech-related violence against women, together with research and solutions from different parts of the world. The campaign offers safety roadmaps and information and provides an avenue for taking action. Take Back the Tech! leads several campaigns at various points in the year, but our biggest annual campaign takes place during 16 Days of Activism Against Gender-Based Violence (25 Nov - 10 Dec).”
    - play on Take back the night
- [Cybersmile](https://www.cybersmile.org/)
    - anti cyberbullying group that provides 24/7 trained support for people who were cyberbullied. They also have educational resources.
- [Digital-Trust](http://www.digital-trust.org/)
    - “Digital-Trust brings together technologists and professionals working with victims and vulnerable people to understand evolving risks and address digital abuse. We provide technical expertise to victims, charities, support groups and organisations working within the criminal justice system as well as directly to people experience digital abuse.”   
- [Online SOS](http://onlinesosnetwork.org/)
    - Online SOS is a nonprofit providing confidential, professional support to individuals experiencing online harassment. They  provide Crisis coaching which includes helping targets of harrasment document their experince and Referral to experts (legal and psychological)
-[Tactical Tech](http://www.tacticaltech.org/)
    - "Founded in 2003, Tactical Tech is a non-profit that has been working worldwide to demystify and promote technology in the context of activism for over a decade. Working at the intersection of tech, activism and politics, Tactical Tech reaches more than three million people worldwide annually through events, training, online resources and exhibitions. We are an international group of technologists, activists, designers and practitioners based in Berlin, who work with citizens, journalists and activists to raise awareness about personal data, privacy and digital security."

    
    
#### Call out groups
 - [Racists Getting Fired](http://racistsgettingfired.tumblr.com/)
   - Blog that posts screenshots of racist posts made on social media along with the poster’s employer’s info so that people contact the company to get them fired
   - [these](http://racistsgettingfired.tumblr.com/tagged/GOTTEN) are the posts that have resulted in successful firings
   - The blog has been [criticized](https://www.washingtonpost.com/news/morning-mix/wp/2014/12/02/racists-getting-fired-exposes-weaknesses-of-internet-vigilantism-no-matter-how-well-intentioned/?utm_term=.77ffbcfddeae) by some for being a form of undiscerning internet vigilantism that on occasion has gotten innocent people fired 
- [Yes You’re Racist](https://twitter.com/yesyoureracist)
  - Twitter account that Identifies and retweets racist content (not a bot). A lot of the tweets they find start off with “I’m not racist but…”
- [Yes You’re Sexist](https://twitter.com/YesYoureSexist)
  - Twitter account that identifies and retweets sexist content (not a bot). A lot of the tweets they find start off with “I’m not sexist but…”. Inspired by @yesyoureracist
- [EverydaySexism](https://twitter.com/EverydaySexism)
  - “Documenting experiences of sexism,harassment and assault to show how bad the problem is &create solidarity.” 
- [Bye Felipe](http://instagram.com/byefelipe)
  - Alexandra Tweten started Bye Felipe on Instagram in October 2014. The idea came from a discussion between women in a private Facebook group about terrible messages women receive from men when they say they aren't interested. Dozens of women posted screenshots of messages they had received and Alexandra decided to gather them all and post them on Instagram.


#### Legal Aid

- [Cyber Civil Rights Initiative](https://www.cybercivilrights.org/welcome/)
  - Founded by Holly Jacobs, CCRI serves serving thousands of victims of Non Consensual Porn (NCP) and advocates for technological, social, and legal innovation to fight online abuse.
  
- [Digital Rights Foundation](https://digitalrightsfoundation.pk) 
  - based in Pakistan, aims to strengthen protections for human rights defenders (HRDs), with a focus on women’s rights, in digital spaces through policy advocacy & digital security awareness-raising.
  - founded by lawyer and human rights activist [Nighat Dad](https://twitter.com/nighatdad) 
  - They have a cyberharassment helpline and between december 1 2016 and may 31 2017 they received 763 complaints in the form of calls emails and facebook messages. This [report](https://digitalrightsfoundation.pk/wp-content/uploads/2017/07/Cyber-Harassment-Helpline-Six-Month-Report.pdf) goes through nature of all these complaints, 
  
- [Without My Consent](http://www.withoutmyconsent.org)
  - WMC provides legal and psychological resources for people who want to fight back against online harassment and privacy violations
  - Founded by lawyer [Erica Johnstone](https://twitter.com/ericajstone)
  - Their twitter page links to good weekly roundup of legal and social news related to non consensual pornography https://twitter.com/withoutconsent 
  - They surveyed 359 people about their experience with online Stalking, Harassment and Violations of Privacy http://www.withoutmyconsent.org/blog/weve-got-data
  
#### Education (Colin)

- [Radio La Benevolencija](www.labenevolencija.org)
  - Radio La Benevolencija Humanitarian Tools Foundation (La Benevolencija) is a Dutch NGO that empowers groups and individuals who are the target of hate speech and ensuing acts. It broadcasts radio soaps, discussions and educational programmes, in combination with grass roots activities that provide citizens in vulnerable societies with knowledge on how to recognise and resist manipulation to violence and how to heal trauma, encouraging them to be active bystanders against incitement and violence. 
  - Uses entertainment that deals with the psychology of incitement to hate and violence in order to inoculate against hate speech and mass conflict
- [HackBlossom](https://hackblossom.org/)
  - Hack Blossom provides educational resources and training for DIY feminist cybersecuirty with a focus on how to secure one's devices and digital identity to prevent and limit domestic abuse and harassment online
  - [DIY Cybersecurity for Domestic Violence](https://hackblossom.org/domestic-violence/)
  - [A DIY Guide to Feminist Cybersecurity](https://hackblossom.org/cybersecurity/)
- [Diverse Gaming Coalition](http://diversegaming.co/)
    - a non profit fighting against online harassment in gaming. Their advocacy includes talks, workshops, streaming, gaming, and partner projects.  
    
- [WomensLaw](womenslaw.org)
    - Website with legal resources about a variety of laws related to women, especially related to domestic violence and other gender-based violence. They have information about cyberstalking and links to ways to stay safe while using technology.

### Initiatives by Social Media Platforms

Most social media sites have a bifurcated approach to presenting their policy on harmful speech to their users. On the one hand, their formal documents such as the terms of service tend to broadly prohibit harassment on the site, but these formal documents rarely elaborate on what constitutes harassment. On the other hand, the websites’ informal documentation such as ‘community guidelines’ will go into more detail about what constitutes misconduct on the website, but falls short of formally defining harassment, giving the websites lee-way to determine what sort of content they remove. In both the formal and informal policy of social media platforms, harassment is usually lumped together with other prohibited activity such as spamming and hacking. 

Source: [Pater, Jessica Annette, et al. "Characterizations of Online Harassment: Comparing Policies Across Social Media Platforms." GROUP. 2016.](http://jesspater.com/wp-content/uploads/2016/12/Pater-Characterizations-of-Online-Harassment-Comparing-Policies-Across-Social-Media-Platforms.pdf)

Many have pointed to the gap between the publicly available policy on harfmul speech and the internal guidelines that websites provide to their content moderators (this is largely thanks to [leaked internal documents](https://www.theguardian.com/news/series/facebook-files)).These internal guidelines are often very detailed, but they are considered arbitrary by critics who argue that the strict criteria results in instances of harmless speech getting removed while some actually harmful speech is left unmoderated. An [often cited example](https://www.propublica.org/article/facebook-hate-speech-censorship-internal-documents-algorithms) of this is Facebook's leaked rules which classified white men as a protected category but not black children. These rules allowed offensive posts targeting black children to stay online and also resulted in the removing of posts by anti-racist activists that addressed white men. Additionally, there is a gap between the written rules (internal or public) and the [actual practices of content moderators](https://www.wired.com/2014/10/content-moderation/) who are often given only a few seconds to decide if a post should be removed.

#### Instagram #KindComments

Instagram (along with Tumblr) seem to be the most proactive in fostering a safe and kind environment on their social media websites. While other websites emphasize their commitment to free expression, instagram instead emphasizes their desire for the platform to be safe, kind, and inclusive. Therefore they don't shy away from asserting thier right as a platform to remove or boost certain posts. On Instagram users can anonymously report live videos and the person live-streaming will receive a notification with resources to mental health hot-lines and other resources. They also have an option in the application for users to “Hide Offensive Comments” which uses a filter to “automatically hide comments on your posts that may be offensive,” and users can also “Hide comments on your posts that contain specific words or phrases.” Instagram also launched a #KindComments campaign where they are encouraging users to generate kind content. Instagram and Tumblr both seem to be more proactive about connecting their users with mental health and anti-bullying resources. 

Source: http://blog.instagram.com/post/162395020002/170629-comments & https://www.wired.com/2017/08/instagram-kevin-systrom-wants-to-clean-up-the-internet/ 
 - Official policies of Facebook, Twitter
 - Twitter's "progress on addressing online abuse"
 - Implementations of Perspective API on Facebook, Reddit
 - New social media outlets (Mastodon) created with these problems in mind
 - Moderation, flagging
 
#### [Gab.ai](https://gab.ai/): The far-right's new social media platform
 - Gab.ai is a social media website founded with the purpose of creating “Free Speech for Everyone.” Their logo is a frog, reminiscent of the alt-right icon Pepe the frog, and they have primarily drawn in far right members with a good deal of racist, sexist, and xenophobic content. It has been described as the far right’s [“digital safe space”](https://www.nytimes.com/2016/11/30/arts/the-far-right-has-a-new-digital-safe-space.html)

source: https://www.theguardian.com/media/2016/nov/17/gab-alt-right-social-media-twitter

### Databases and Datasets

Some organizations maintain structured databases of abuse. The No Hate Speech Movement's [Hate Speech Watch database](https://www.nohatespeechmovement.org/hate-speech-watch/instructions) is a database for user-submitted reports of hate speech on the Internet. The database contains descriptions of websites containing hate speech, social media posts, and abusive users on social media. The website [Trolldor: the global blacklist of twitter trolls](https://www.trolldor.com/) similarly collects data on Twitter trolls, and maintains a "top 10 worldwide trolls" list. [Hatebase.org](https://hatebase.org), which bills itself as the "world's largest online repository of structured, multilingual, usage-based hate speech," is a database of terms or phrases that its users report as hate speech. Although not all of these databases seem to have public APIs, if their data might nonetheless be used to train language or metadata categorizers. 

Public datasets also exist which might be used as training data for categorizers.  Many authors of papers in this area of computational linguistics also release their training data, much of which is manually labeled [@kolhatkar_constructive_2017, @ott_finding_2011, @samghabadi_detecting_2017, @waseem_hateful_2016, @wulczyn_ex_2017]. For [an analysis of the Gamergate controversy](https://prpole.github.io/semantic-analysis-of-one-million-gamergate-tweets/), Phillip Polefrone collected a dataset of roughly one million tweets. The 2012 Kaggle task provides [a CSV with hand-labeled abusive short messages](https://www.kaggle.com/c/detecting-insults-in-social-commentary).The Wiki DeTox project of the Wikimedia Foundation provides language from their [Wikipedia Talk Pages](https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973), human-annotated for "toxicity" and "aggression." A separate dataset of theirs provides [personal attack annotations](https://figshare.com/articles/Wikipedia_Detox_Data/4054689) of over 100K Wikipedia comments, manually annotated by about 4,000 annotators. 

Finally, organizations such as [Bing and Google maintain public "bad word lists."](http://www.slate.com/articles/technology/future_tense/2013/08/words_banned_from_bing_and_google_s_autocomplete_algorithms.html) Shutterstock, for instance, unofficially maintains a "List of Naughty, Obscene, and Otherwise Bad Words" in 25 languages, including Esperanto and Klingon. These lists are currently used to filter and automatically replace offensive language, and might be used to inform feature vectors for categorizers.

### Organizations and Projects Employing Machine Learning 

A few projects and institutions already use a machine learning categorization techniques to identify abusive language. Arguably the most well-known of these is the Google incubator [Jigsaw](https://jigsaw.google.com/), whose self-described mission is to "tackle some of the toughest global security challenges facing the world today—from thwarting online censorship to mitigating the threats from digital attacks to countering violent extremism to protecting people from online harassment." Their project [Perspective](https://www.perspectiveapi.com/) provides an API for querying a "toxicity" language model, one trained on data manually labeled on a spectrum from "very healthy" to "very toxic." They define "toxic" as "a rude, disrespectful, or unreasonable comment that is likely to make you leave a discussion." Much of their code [is freely available online, on GitHub](https://github.com/conversationai). Among these code repositories are moderator support programs, that use the Perspective API and automatically label contributions for toxicity on comment platforms such as [Reddit](https://github.com/conversationai/conversationai-moderator-reddit), [WordPress](https://github.com/conversationai/conversationai-moderator-wordpress), and [Discourse](https://github.com/conversationai/conversationai-moderator-discourse).
[Those that have tested the API, however, report mixed results,](https://qz.com/918640/alphabets-hate-fighting-ai-doesnt-understand-hate-yet/) a fact which has led one commentator to posit that the AI ["mistakes civility for decency"](https://motherboard.vice.com/en_us/article/qvvv3p/googles-anti-bullying-ai-mistakes-civility-for-decency). 

The related Wikimedia Foundation project [Detox](https://meta.wikimedia.org/wiki/Research:Detox) publishes previously mentioned public data sets, and also provides an API for querying two of their models: of personal attacks and aggressive tone [@wulczyn_ex_2017]. Our tests of the API, like those of Perspective above, show problems handling ambiguity: the threatening phrase "be careful, you might find some white powder in an envelope come in the mail one day" was rated as 1% aggressive, while the more playfully celebratory "I'm going to nominate you for the Nobel prize, you brilliant man" was rated 61% aggressive.

The Mozilla-funded [Coral Project](https://coralproject.net/) publishes a comment moderation assistance tool called [Talk](https://blog.coralproject.net/talk-features/), in use by the New York Times, in which language containing banned words is rejected, and suspect language is flagged or highlighted to aid moderators. It is unclear whether the project uses machine learning techniques or simply a dictionary-based approach, however.  

More proprietary systems also exist. A number of US patents---US5796948, US8868408, US8473443, US7818764, and US20080109214---describe "offensive message interceptors," "methods for word offensiveness processing," "inappropriate content detection," "methods for monitoring blocked content," and "methods for computerized psychological content analysis," respectively.

## Computational Detection of Abusive Language, Behaviors, or People (Jonathan)

### General Classification Studies

Most computer science and computational linguistics studies in the detection of abusive language treat the problem as one of document categorization. This is a very common pattern of machine learning analysis in which an algorithm decides whether to place documents in one of two or more categories. These categories are almost never identically defined across studies, but are variously termed "abusive / non-abusive," "hate speech / non-hate speech," "high-quality / low-quality," and so on. 

Almost all of the categorization experiments we examined employ some form of supervised machine learning. Supervised learning approaches involve training a machine learning algorithm on a set of pre-labeled training data, such as tweets or comments that human annotators have labeled as "abusive" or "non-abusive."  The algorithm learns which textual or metatextual features best correlate with their categories, and then uses these weighted features to predict the probability of an unlabeled document falling into a category. The studies we examined vary greatly in their approaches: the features they choose, the classification algorithms they use, and the categories themselves. 

There are two main categories of features used in these studies: textual and metatextual features. Textual features are properties of the abusive language itself, without reference to their context; metatextual features are contextual: users' self-descriptions, IP addresses, and operating systems, just to name a few. Some document categorization studies employ only textual features (which, in rare cases, are the only features available), but most employ a mix of both. 

Textual features are either words, characters, or groupings of words or characters known as n-grams, where n refers to the number of words or characters. Character 4-grams, for instance, refers to groupings of four characters, and word trigrams refers to groups of three words. Skip-trigrams refers to groups of words representing every other, or every third word in a series.  

Choosing what constitutes a word, and defining its boundaries, is an important first step in the creation of textual features called tokenization. This may seem like an inconsequential first step in text analysis, and is for this reason often considered _pre_-processing, but careful tokenization, informed by domain-specific knowledge and familiarity with the textual corpus, can make significant differences in the outcomes of the categorization experiment. A related task, termed normalization, involves transforming words into their canonical morphologies, either through lemmatization (collapsing words into their dictionary forms, i.e. "went" to "ran"), or stemming (transforming word variants into their stems, i.e. collapsing "translate" and "translation" into "translat"). 

Normalization often also involves domain- or platform-specific transformations. Treating emoji as words, for instance, or transforming them into words with sentiment valence, often greatly influences the outcomes of sentiment analysis. (See, for example, [@castillo_predicting_2013, samghabadi_detecting_2017]). Collapsing obfuscated words, like transforming "w o r d" into "word," and "ta$k into "task," also proved to be useful to participants in the Kaggle contest, [Detecting Insults in Social Commentary](https://www.kaggle.com/c/detecting-insults-in-social-commentary). Related normalization techniques involve expanding abbreviations such as "r u" into "are you," and collapsing long vowel representations, transforming "coooooool" into "cool." 

The results of this tokenization and normalization are language features in the form of n-grams. These features are then vectorized and weighted using a variety of techniques. In some cases, binary representations of words are used (either 1 or 0 for the presence or absence of a word in a document, see [@sood_automatic_2012]), but more frequently, term frequencies are used (ratios of the words in each document), and even more frequently, TF-IDF, or term frequencies adjusted for inverse document frequencies (see [@samghabadi_detecting_2017, @diakopoulos_editors]). These vectorizations are often used in statistical studies of linguistic style (stylometry) and are sometimes considered proxies for the stylistic fingerprints of individual or authorial voices. However, the limitation of these vectors is that they require a critical mass of text (>500 words) for term frequencies to be statistically practical. 

Other vectorization techniques take into account the meanings or functions of the words. Word embeddings, for instance, transform words into high-dimensional vectors that encode the probabilities of their co-occuring with other words in a large corpus. Embeddings from the [Stanford GloVe vectors](https://nlp.stanford.edu/projects/glove), for instance, encode semantic information into high-dimensional vectors. 

 - POS representations (and in detection of quality, detection of psycholinguistics)
 
To this collection of token vectors, often other language measurements are added. General measurements such as document length are usually among these features. More specific measurements may include ratios of capital letters (a proxy for all-caps emphasis) and ratios of punctuation marks such as exclamation points. (The features used by [the 2014 Stanford Literary Lab Pamphlet 7](https://litlab.stanford.edu/LiteraryLabPamphlet7.pdf), "Loudness in the Novel", are similar, and are used as proxies for what they term "loudness.")

 - Features used 
 - Classifiers used
 - Meta-classification

### Detection of Quality, Formality

Some related content-based approaches to language categorization include the identification of language quality. "Quality" can refer to the subjective usefulness of speech towards the goals of a particular website or online community, grammatical quality (correctness), credibility, or formality, among other definitions. [@siersdorfer_how_2010] define quality as YouTube comments with good feedback (high numbers of user upvotes), and construct a categorization experiment where 6.1M training comments are vectorized using the most distinctive words of each category (TF-IDF), their SentiWordNet sentiment synonyms, and then classified using support vector machine classifiers. [@agichtein_finding_2008] also define quality based on user-reported and metadata-based reputation scores, like PageRank and ExpertiseRank, in an experiment categorizing Yahoo Answers conteng. They test a number of features of each answer, including n-grams of length 1-5, their POS representations, and metadata such as number of clicks, and categorize these using stochastic gradient boosted trees.

A related metric is language formality. [@heylighen_variation_2002] propose a formality-score based on proportions of parts of speech. Based on the anthropologist Edward T. Hall's concept of "high-context" and "low-context" speech, and on the linguistic concept of deixis, they divide their lexicon into deictic words, such as pronouns, adjectives, and interjections, and non-deictic words, such as nouns and adjectives. The formality score is then proportional to the sum of deictic word frequencies subtracted from the sum of non-deictic word frequencies. This score is used, though not successfully, in a categorization experiment in [@agichtein_finding_2008], who also use a variety of other language quality metrics, like grammatical correctness. Similar measurements, like spelling, uppercase frequency, and lexical entropy, are used for comment classification in [@brand_comment_2014]. 

The presence of profanity has also been shown to correlate with abusive language, as in [@brand_comment_2014]. Many of the top entries in the Kaggle contest, for instance use a "bad words list" as a seed feature set. [@samghabadi_detecting_2017] uses Google's bad words dictionary, and combines it with a list from another researcher. "Bad" words themseles, though, are of course never unambiguously bad. [@kapoor_swears_2016], for instance, describes an attempt to differentiate between "casual" and abusive swearing. They find that the high-severity swears are likely to occur in abusive contexts. 

### Sentiment Analysis

### Metadata Analysis

While many of the computational approaches described thus far have been concerned with detection of abusive speech through content analysis, metadata provides, in many cases, an even more useful feature set for categorization. [@castillo_predicting_2013], for instance, find that the presence of a Twitter user's self-description ("bio") correlates strongly with the likelihood of their authoring abusive tweets. [@agichtein_finding_2008], in their analysis of Yahoo answers social relations, find social network analysis and trust propogation to be useful in predicting the quality of an answer.  

 - Detection of bots, trolls
 - Social network theory
 - Trust propogation

### Related Fields

Credibility is a similar index. Using a training corpus of tweets labeled by volunteers as likely or unlikely to be true, [@castillo_predicting_2003] test a variety of features and methods for classification. 

 - Detection of deceptive opinion spam 
 - Detection of misinformation

## And the Law (Colin)
### US Federal Law
- section 230 of the Communications Decency Act
- Civil Rights Law
-[Elonis v United States](https://www.supremecourt.gov/opinions/14pdf/13-983_7l48.pdf)
  - [menacing behavior online thrown out by supreme court](http://www.pewresearch.org/fact-tank/2015/06/01/the-darkest-side-of-online-harassment-menacing-behavior/) 
### European Law
- [Code of Conduct on Countering Illegal Hate Speech Online](ec.europa.eu/justice/fundamental-rights/files/hate_speech_code_of_conduct_en.pdf)
- [Code of Conduct signed by Facebook, Microsoft, Twitter and YouTube](https://www.theguardian.com/technology/2016/may/31/facebook-youtube-twitter-microsoft-eu-hate-speech-code)
### International Law
- International Covenant on Civil and Political Rights (ICCPR)

## Counterspeech (Colin)

Counter Speech can address the perpetrator of hate speech or the cyber bystanders whose discourse norms are shaped by seeing hate speech go uncontested. For hateful speakers with deeply ingrained hate, counter speech is less effective, but it can still alter their discourse to be less overtly hateful which in itself can also positively affect the cyber bystanders.

The guiding principle of counterspeech is the liberal ideal that more speech is the best remedy to harmful speech. This is partially informed by the idea that if you delete hateful content from one platform that there is always somewhere else to go (I’m not sure this is totally convincing).

### Recommended Strategies
- Warning of consequences
  - Remind speaker of harm done by speech
  - Remind of offline consequences and the permanence of online communication
  - Remind of online consequences (blocking, reporting, suspended account)
  - Mainly effective at getting hate speech deleted and doesn't necessarily change speakers POV
- Shaming and Labeling
  - Labeling the speech (not speaker) as bigoted, misogynist, etc
  - Helpful to cyberbystanders
  - Speaker ‘may not have known’ so it is better not to make a personal attack and to only to label the speech has harmful
- Empathy and Affiliation
  - Change the tone to friendly, empathetic or peaceful
  - Affiliate with speaker and establish a connection (ex: I am also a conservative, but…)
  - Affiliate with targeted group (ex: what you said hurt me as an asian…)
  - Changing the tone is more effective when the speaker affiliates with counterspeaker
- Humor
  - Neutralize hateful speech that is seen as dangerous and intimidating
  - Attract larger audience to the counterspeech’s message
  - Use humor to soften the message of counterspeech that could otherwise come off as hostile or aggressive (ex: [it’s time to stop posting cat](http://images.wikia.com/glee/images/c/c7/Imgur_gallery_hZSfl_It%27s_Time_to_Stop_Posting.jpg))
- Images
  - Can make counterspeech more viral
  - Counterspeech is generally more effective when it is emotive rather than rational/logical so images can be a good way to “send people along emotive pathways”

### Discouraged Strategies
- Hostile or aggressive tone and insults
  - Can cause backfire or speaker to dig in their heels
- Fact-checking
  - Fact checking may sway cyber bystanders, but it is unlikely to influence original speaker
  - Speaker will find a way to fit the new facts presented to the conclusions they are already committed to
    - Social psychologists call this the backfire effect where challenging someone’s views with facts leads them to hold those views even more firmly
  - Pointing out hypocrisy can be good for bystanders but usually not for speaker 
- Harassment and Silencing

### Successful counterspeech is indicated by: 
- Speaker shifts their discourse if not also their beliefs 
- Speaker apologizes, recants, or deletes original hate speech
- Discourse norms of the cyberbystanders are positively affected
- Hate speech narratives delegitimized (even if speaker is not swayed)
- More counter speech is elicited from the audience (this is good until it turns into harassment/dogpiling) 

### Types

A useful method of typologizing harmful speech online is by distinguishing between the types of exchanges (vectors). (This is based on the models put forward in Counterspeech on Twitter: A Field Study)
 
 - One-to-one: one person deploying counterspeech against one person’s hate speech
 - One-to-many: one person deploying counterspeech against many people’s hate speech
 - Many-to-one: many people deploying counterspeech against one person’s hate speech
 - Many-to-many: many people deploying counterspeech against many people’s hate speech
 
Source: 
- [Counterspeech on Twitter: A Field Study, by Susan Benesch, Derek Ruths, Kelly Dillon, Haji Mohammad Saleem, and Lucas Wright](https://dangerousspeech.org/counterspeech-on-twitter-a-field-study/)
- [Considerations for Successful Counterspeech,  by Susan Benesch, Derek Ruths, Kelly Dillon, Haji Mohammad Saleem, and Lucas Wright](https://dangerousspeech.org/considerations-for-successful-counterspeech/)

 
###  Kevin Munger's Study: [Tweetment Effects on the Tweeted: Experimentally Reducing Racist Harassment](https://link.springer.com/article/10.1007/s11109-016-9373-5)
 
 Kevin Munger conducted a study where he identified 231 twitter accounts operated by white men that regularly used racial slurs against black twitter accounts. Munger created a variety of twitter ‘bots’ with different amounts of followers, and some with a white male avatar and others with a black male avatar. The bots would send the following tweet in response to  detected racist tweets by the identified twitter accounts: "@[subject] Hey man, just remember that there are real people who are hurt when you harass them with that kind of language.”

Munger found that accounts confronted by the white male twitter bot with a lot of followers were most likely to alter their language in future posts. Around 27% of users stopped using the n-word in their posts the following weeks. Munger’s findings corroborate existing theories on counterspeech that tell us that counterspeech is most effective when done by someone with whom the harmful speaker identifies (in this case white males identify with other white men). On the other hand, Munger’s findings cut against existing theories about how anonymity affects online behavior; the study found that accounts with personal information (name, photo, location, etc.) that used the n-word were significantly less likely to change their behavior after being rebuked by the bot, and in some cases would increase their usage. It was primarily accounts with little or no personal information that would redress their behavior. This goes against the theory that anonymity contributes to online hate speech, since those that were most anonymous were the most likely to alter their behavior. 

 
## Inoculation 
 Inoculation is a long-term method for fighting against hate speech that takes some time. It involves instilling values in a society that oppose hate speech, and deals especially with building the social-psychological tools necessary so that groups of people don't fall victim to the pressures of engaging in hate speech or being incited by it. 
 - An example of a group that deals with Inoculation is Radio la Benevolencija (RLB) a dutch nonprofit that produces entertainment for countries in central africa that deals with the psychology underlying incitement to hate and violence.
 - [Citron and Norton](http://web.a.ebscohost.com.ezproxy.cul.columbia.edu/ehost/pdfviewer/pdfviewer?vid=1&sid=1c840371-b7ff-4ba7-a74e-8848b5bae30a%40sessionmgr4008) suggest that internet intermediaries and society at large  (especially public schools) play a stronger role in fostering digital citizenship   

## Calling out & doxxing
 - Gettign racists fired
 - Yes you’re racist/sexist
 - Goodbye felipe
# Future Directions
## Potential Applications of Related Fields (Jonathan)

### Psycholinguistics

 - Linguistic properties of emotional speech
 - "Language of psychopathy"

### Fusions of Existing Approaches 

 - Formality detection and social network theory
 - Quality ranking

## Automated Counterspeech 
