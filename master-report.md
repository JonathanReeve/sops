---
title: A Safer Online Public Square
authors: Jonathan Reeve, Colin Muller
---

# Problems

## Introdution 

Perhaps the most challenging step in developing social or technological tools for promoting a ‘safer online public square’ is defining what sort of speech constitutes a threat to the civility and safety of members of online communities.

Where do we draw the line between off-color and offensive content and content that inflicts harm or hate upon an individual or group? Additionally, how can a third party moderator (or algorithm) gauge the context of the speech as well as the the subjective perception of the speech by both speaker and target of the speech? 

Some critics of attempts to filter out harmful speech online claim that it violates first amendment free speech principles. These arguments vary, but generally point to the danger that   content moderation may silence dissenting voices or unpopular opinions. On the other hand, defenders of content moderation assert that  social media platforms have the right to remove content at their own discretion as they are not government agents who are held to first amendment standards. First amendment legal scholars have split the debate of free speech and content intermediaries between [the right to speak and the right to hear/be heard](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1205674) , with some arguing that the mere right to speak is insufficient if the media has the power to suppress any platform that this speech may have. Conversely, an individuals ability (or right?) to rapidly draft and publish a hateful tweet does necessitate that those targeted by the speech must view the harmful post.
 - Other concerns about moderation include threats to anonymity and privacy, no platforming, [silencing marginalized voices](https://www.propublica.org/article/facebook-hate-speech-censorship-internal-documents-algorithms), [amplifying hate speech by calling it out](http://archives.cjr.org/minority_reports/reprint_reporting_and_race.php), and fixing virtual identites to legal identities through ['real name' policies](https://www.theguardian.com/world/2017/jun/29/facebook-real-name-trans-drag-queen-dottie-lux)

There are many categories of speech that fall under the umbrella of harmful speech (The main categories are harassment, bullying, hate speech, and dangerous speech), but the boundaries between these different categories are neither rigid nor clear. The ambiguity posed by these various categories has led some to abandon formal definitions and seek different approaches to classifying harmful speech (see section below). Beyond this issue of classifying speech, there remains the problem of what one subjectively perceives as harmful speech; one [study](https://pdfs.semanticscholar.org/db55/11e90b2f4d650067ebf934294617eff81eca.pdf) found only 33% overall agreement between students of different races asked to assess the degree to which tweets were racially offensive.

### Statement of purpose of our preliminary report (to discuss at meeting)

## Taxonomies

There is significant corpus of writing by scholars attempting to formulate a definition and taxonomy of harmful speech online. These classifications will often vary based on the purpose of the definition which varies from academic research, legal recommendations, or advocacy. These different purposes result in varying breadth and scope of definitions, and disagreement over definitions is one of the main challenges in compiling data from different sources on the frequency of harmful speech online (as the specific behavior being monitored in different studies varies widely). Despite these challenges, it is useful to outline the different categories of harmful online behavior, as these different types manifest themselves differently and will require different approaches for intervention.

 - Hate speech
   - “An expression that denigrates or stigmatizes a person or people based on their membership of a group that is usually but not always immutable, such as an ethnic or religious group. Sometimes other groups, defined by disability or sexual orientation, for example, are included.” Source: Benesch, Susan. Defining and diminishing hate speech. 2014. 

 - Bullying:
   - “Cyberbullying is any behavior performed through electronic or digital media by individuals or groups that repeatedly communicates hostile or aggressive messages intended to inflict harm or discomfort on others. Additionally, the following addendum may be included with the definition.... In cyberbullying experiences, the identity of the bully may or may not be known. Cyberbullying can occur through electronically mediated communication at school; however, cyberbullying behaviors commonly occur outside of school as well." Tokunaga, Robert S. "Following you home from school: A critical review and synthesis of research on cyberbullying victimization." Computers in human behavior 26.3 (2010): 278.
   - Definitions of cyberbullying are derived from definitions of traditional bullying which generally include the 3 conditions of 1. Repeated behavior 2. Psychological torment 3. Carried out with intent.
     - the question of the repetition of bullying behavior is complicated online since a single post on a social media site can have a repeated effect on its target with every ‘like’ or ‘share’ 
   - Cyberbullying is seen more as an “opportunistic offense” since individuals who don’t engage in traditional bullying will do so online since of the low threat of being caught and the lack of any clear disciplinary authority

 - Harassment and stalking
   - "Online harassment includes a wide range of targeted behaviors including: threats, continued hateful messages, doxxing, DDoS attacks, swatting, defamation, and more. Online harassment can target (or come from) a group or individual and often has the expressed purpose of having the individual or group leave the internet, take down their content, or to dissuade them from publically having a point of view. While there is space for debate and discussion online (as well as conflicting ideas!), what separates online harassment from healthy discourse is the focus on harm: including publishing personal information, sending threats with the intention to scare or harm, using discriminatory language against an individual, and even directly promoting harm against a person or organization."
Source: [HeartMob](https://iheartmob.org/pages/faqs#online-harassment)
 - Dangerous speech
   - “speech that can inspire or catalyze intergroup violence” Source: [Counterspeech on Twitter: A Field Study](https://dangerousspeech.org/counterspeech-on-twitter-a-field-study/) 
   - Susan Benesch puts forward the following conditions that make the likelihood of speech resulting in group violence : 
     - there is a “powerful speaker with a high degree of influence;”
     - there is a receptive audience with “grievances and fear that the speaker can cultivate;”
     - a speech act “that is clearly under-stood as a call to violence;”
     - a social or historical context that is “propitious for violence, for any of a variety of reasons;”
     - An “influential means of dissemination.”” 
     - Source: http://www.worldpolicy.org/sites/default/files/Dangerous%20Speech%20Guidelines%20Benesch%20January%202012.pdf 

 - Abuse of journalists
   - "Gamergate"
 - Related language patterns that, while not strictly abusive language, could be useful to its detection
   - Misinformation (e.g. Russian fake news)
   - Deceptive opinion spam (e.g. fake Amazon product reviews) 
   
### Alternative approaches to classifying harfmul speech:

Besides the approach of defining and taxonomizing different forms of hate speech, some have formulated other vectors for recognizing harmful speech online. 

#### Implicit v. Explicit & Generalized v. Directed Abusive Language 
   - Waseem et al. propose two primary factors for typologizing abusive language rather than attempting to define various terms such as abusive language, hate speech, cyberbullying, cyber harassment. They propose the following two factors: 
     - Is the language directed towards a specific individual or entity or is it directed towards a generalized group?
     - Is the abusive content explicit or implicit?     
   - Typologizing based on these two factors is useful, as there can be a lot of overlap and ambiguity when relying on stricter defintions. One shortcoming of this typology is that by making the distinction between directed/generalized attacks, the research may downplay the fact that hate speech, even when verbally directed at an individual (ex: calling someone a racial slur) is a crime against an entire sub-group of people. 
   - Source: [Waseem, Zeerak, et al. "Understanding Abuse: A Typology of Abusive Language Detection Subtasks." arXiv preprint arXiv:1705.09899 (2017).](http://www.aclweb.org/anthology/W17-3012)

#### Justice Stewart's rule: “I know it when I see it”
Justice Stewart famously asserted that “I know it when I see it” when referring to identifying obscenity. It seems to be the consensus that this approach is not applicable to identifying hate speech due to the variety of forms of speech and contexts which one could identify as hate speech.

There are instances where specific epithets or insults are used and an outsider or scholar may see hate speech but the speaker/recipient do not. Henry Louis Gates Jr. for this reason asserted that we should not “spend more time worrying about speech codes than coded speech.” Since a lot of hate speech can be coded or masked by symbols.

The discussion of Stewart’s “I know it when I see it” points to a central difficulty in defining hate speech since it requires assessing the subjectivity and intention of both the perpetrator and the victim. However, only some definitions include the component of intention on the part of the perpetrator, and definitions also vary on how they define harm to the victim.

#### What is left out of definitions

- Harmful speech directed at one's self
  - this includes online posts that depict eating disorders or suggest self-harm. It is infrequently included in discussion of harmful online behavior, but it is a serious issue that needs to be dealt with in a different way than outward oriented harmful speech. 
- Harmful speech between a group that does not include the target of the speech
  - An example of this could be a private message channel where racists exchange racist messages about a group of people who are not a part of the message channel. Upon first consideration this speech may seem more like thought than it does speech in the public realm, but such conversation can have real life consequences on the well being of those discussed such as in private [message forums where doxxing information is collected](https://theintercept.com/2017/09/06/how-right-wing-extremists-stalk-dox-and-harass-their-enemies/) 

## Statistics 

 - General stats related to abusive langauge
 - Cyberbulling stats
 - Hate speech stats

[A 2014 Pew Research Center survey](http://www.pewinternet.org/2014/10/22/online-harassment/) found that 73% of adult Internet users have witnessed harassment online, and 40% have experienced it personally [@duggan_online_2014]. [A later 2016 report by the Data and Society Research Institute](https://www.datasociety.net/pubs/oh/Online_Harassment_2016.pdf) claims that, of American Internet users, 72% have witnessed harassment or abuse, and almost half (47%) have personally experienced it [@lenhart2016online]. 

The types of harassment include calling of names (reported by 60% of witnesses)  purposeful embarrassment (53%), physical threats (25%), sexual harassment (19%), and stalking (18%) [@duggan_online_2014]. Of those who had personally experienced harassment, 8% had been physically threatened or stalked, and 6% had been sexually harassed. These typically took place on social media platforms, although also in comments sections or in multiplayer games. 

The Pew report finds that young women, aged 18-24, are disproportionately targeted in all categories except for those of purposeful embarrassment and the calling of offensive names. [A 2013 report by the WHOA organization](http://www.haltabuse.org/resources/stats/index.shtml) (Working to Halt Online Abuse) echoes this finding. In their analysis of 4,043 self-reported cases of abuse in American from 2000-2013, they find that 70% of victims were female, with a 42% majority between the ages of 18 and 30. The abusers, they find, are more likely to be men (47%) than women (30%). LGB Internet users, as well, are more likely to experience harassment [@lenhart2016online 37]. 

## Potential Causes 

While theory building on the underlying causes of harmful speech online is generally underdeveloped and often not rigorously proven with empirical research, ([Tokunaga 2010](http://www.sciencedirect.com/science/article/pii/S074756320900185X)), existing social-psychological theories helps us better determine effective forms of intervention. This vein of research is particularly useful in emphasizing the impact that an individual’s harmful speech can have on their social group, since “cyberbystanders” witnessing of abusive language online impacts their  understanding of acceptable online norms. This research then reminds us that when intervening in the name of a safer online public sphere, it is not only the speaker and recipient that we must pay attention to, but also those bystanders and digital onlookers who may also happen to witness the encounter. 

### Anonymity: online disinhibition, deindividuation, and depersonalization

- Suler's theory of the [online disinhibtion effect](db19.linccweb.org/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=13621589&site=ehost-live)
  - Suler’s theory of the “online disinhibition effect” argues that online users compartmentalize their “online self” and “real life self” and the normal cognitive processes that guide their “real life” behavior are suspended when they are online. Online disinhibition is  a result of anonymity, asynchronous communicaiton, and empathy deficit.
- [Barlett and Gentile Model](http://www.apa.org/pubs/journals/features/ppm-ppm0000055.pdf)
  - Similar to Suler’s theory, Barlett and Gentile Model argues that attitudes towards cyberbullying mediate the relationship between anonymity and likely cyberbullying behavior since users who realize their anonymity online will dissociate their online actions from their “real” self which will contribute to positive cyberbullying attitude and increased cyberbullying. The model hypothesizes that perceived anonymity gives potential cyberbullies a sense of impunity online as well as empowerment since they can attack individuals they would not be able to attack offline either because of differences in physical strength or because they do not actually know or live near that person offline. Perceived anonymity leads cyberbullies to distance themselves from their own actions and gain a more positive perception of cyberbullying thus creating a feedback loop that encourages further cyberbullying.
  - Barlett and Gentile propose intervening against cyberbullying by informing “Internet users that they are not anonymous and show them evidence of IP address tracking and how History folders operate, then perhaps cyberbullying will decrease”  (178). However this form of intervention would not be effective in cases where users are actually able to be anonymous online (such as by using a mix network architecture).
  - A limitaiton to theories that argue that anonymity contributes to harfmul online behavior (such as those by Suler, Barlett, and Gentile) is the ambiguous definition of anonymity that doesn’t distinguish between pseudonymity, perceived anonymity, or partial anonymity.
- deindividuation V. Depersonalization 
  - Suler and Barlett and Gentile's theories suggest that a user online undergoes a sort of deindividuation, where they experience a loss of their sense of self. Other researchers have pushed back against the idea that online activity leads to deinidivuation and instead have put forward the notion of depersonalization as a better explanation. Depersonalization is defined as a “process through which individuals perceive that their certain group identity is more salient than other identities in a particular context, termed as ‘the emergence of group in the self’ (399). The principle difference is that deindividuation implies a loss of rationality that leads to necessarily harmful results whereas depersonalization is stripped of some of the value judgments and provides an explanation for both positive and negative group behavior online. Huang et al in their study "The effect of anonymity on conformity to group norms in online contexts: a meta-analysis” put forward an argument for depersonalization online as they demonstrate that perceived anonymity results in group conformity online. 
  -  By understanding how anonymity online can lead to greater group conformity, we can see how this dynamic can be used to combat (as well as contribute to) harmful speech online. If an individual is part of an online community where harmful speech is perceived as unacceptable and uncommon, then they are less likely themselves to engage in harfmul speech.
    - A clear example of depersonalizaiton or ‘mob mentality’ being mobilized to combat harfmul speech online is the website [HeartMob](https://iheartmob.org/about) which allows for online bystander intervention and provides an immediate support group for people who have faced online harassment. 
 - depersonalization: prioritizing one's identity with the group over one's individualized identity
### Mimicry Effect
 - This theory holds that boosting positive content rather than deleting harmful content will foster an environment where other members of the website will contribute positive content
 
### Backfire effect
 - Political psychologists Nyhan and Reifler have researched the “backfire effect” where attempts at correcting misperceptions or misinformed beliefs results in firmer beliefs in the misperception or misinformation
   - “individuals who receive unwelcome information may not simply resist challenges to their views. Instead they may come to support their original opinion even more strongly”  Nyhan, Brendan, and Jason Reifler. "When corrections fail: The persistence of political misperceptions." Political Behavior 32.2 (2010): 307.
   - It is important to consider the backfire effect in looking at effective forms of intervention/counterspeech, since often presenting facts or engaging in logical/reasoned debate is not the most effective strategy
   - A similar psychological phenomenon is “motivated reasoning” where people make a strong effort to support the conclusions they seek despite being exposed to contradictory facts
   
### Temporal clustering of hate speech
  - prejudicial crimes are strongly influenced in the short term due to publicized events such as murders committed by a minority, this amplification in hate speech usually lasts about 2 weeks.  King, R.D., and G.M. Sutton. 2013. “High Times for Hate Crime: Explaining the Temporal Clustering of Hate Motivated Offending.” Criminology 51 (4): 871–94
  - Hashtags can be a good indicator of temporal clustering: ex: #killallmuslims #ferguson #charliehebdo #brussels #banislam #baltimore #mizzou

# Existing Approaches to Intervention
## Organizations 
### Advocacy Groups (Colin and Jonathan)

 - Working to Halt Online Abuse
 - No Hate Speech Movement
 - Etc...

A number of organizations exist to study and combat harassment, hate speech, and related phenomena. The [Women's Media Center Speech Project](http://wmcspeechproject.com/) studies and reports on "the underrepresentation and misrepresentation of women in the media," including online harassment of women. 

#### Legal Aid (Colin)
#### Education (Colin)

### Initiatives by Social Media Platforms

Most social media sites have a bifurcated approach to presenting their policy on harmful speech to their users. On the one hand, their formal documents such as the terms of service tend to broadly prohibit harassment on the site, but  rarely elaborate on what constitutes harassment. On the other hand, the websites’ informal documentation such as their ‘community guidelines’ will go into more detail about what constitutes misconduct on the website, but falls short of formally defining harassment, giving the websites lee-way to determine what sort of content they remove. In both the formal and informal policy of social media platforms, harassment is usually lumped together with other prohibited activity such as spamming and hacking. 

Source: [Pater, Jessica Annette, et al. "Characterizations of Online Harassment: Comparing Policies Across Social Media Platforms." GROUP. 2016.](http://jesspater.com/wp-content/uploads/2016/12/Pater-Characterizations-of-Online-Harassment-Comparing-Policies-Across-Social-Media-Platforms.pdf)

Many have pointed to the gap between the publicly available policy on harfmul speech and the internal guidelines that websites provide to their content moderators (this is largely thanks to [leaked internal documents](https://www.theguardian.com/news/series/facebook-files)). Furthermore, there is a gap between the written rules (internal or public) and the [actual practices of content moderators](https://www.wired.com/2014/10/content-moderation/) who are often given only a few seconds to decide if a post should be removed.

 - Official policies of Facebook, Twitter
 - Twitter's "progress on addressing online abuse"
 - Implementations of Perspective API on Facebook, Reddit
 - New social media outlets (Mastodon) created with these problems in mind
 - Moderation, flagging

### Databases and Datasets

Some organizations maintain structured databases of abuse. The No Hate Speech Movement's [Hate Speech Watch database](https://www.nohatespeechmovement.org/hate-speech-watch/instructions) is a database for user-submitted reports of hate speech on the Internet. The database contains descriptions of websites containing hate speech, social media posts, and abusive users on social media. The website [Trolldor: the global blacklist of twitter trolls](https://www.trolldor.com/) similarly collects data on Twitter trolls, and maintains a "top 10 worldwide trolls" list.  [Hatebase.org](https://hatebase.org), which bills itself as the "world's largest online repository of structured, multilingual, usage-based hate speech," is a database of terms or phrases that its users report as hate speech. Although not all of these databases seem to have public APIs, if their data might nonetheless be used to train language or metadata categorizers. 

Public datasets also exist which might be used as training data for categorizers.  Many authors of papers in this area of computational linguistics also release their training data, much of which is manually labeled [@kolhatkar_constructive_2017, @ott_finding_2011, @samghabadi_detecting_2017, @waseem_hateful_2016, @wulczyn_ex_2017]. For [an analysis of the Gamergate controversy](https://prpole.github.io/semantic-analysis-of-one-million-gamergate-tweets/), Phillip Polefrone collected a dataset of roughly one million tweets. The 2012 Kaggle task provides [a CSV with hand-labeled abusive short messages](https://www.kaggle.com/c/detecting-insults-in-social-commentary).The Wiki DeTox project of the Wikimedia Foundation provides language from their [Wikipedia Talk Pages](https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973), human-annotated for "toxicity" and "aggression." A separate dataset of theirs provides [personal attack annotations](https://figshare.com/articles/Wikipedia_Detox_Data/4054689) of over 100K Wikipedia comments, manually annotated by about 4,000 annotators. 

Finally, organizations such as [Bing and Google maintain public "bad word lists."](http://www.slate.com/articles/technology/future_tense/2013/08/words_banned_from_bing_and_google_s_autocomplete_algorithms.html) Shutterstock, for instance, unofficially maintains a "List of Naughty, Obscene, and Otherwise Bad Words" in 25 languages, including Esperanto and Klingon. These lists are currently used to filter and automatically replace offensive language, and might be used to inform feature vectors for categorizers.

### Organizations and Projects Employing Machine Learning 

A few projects and institutions already use a machine learning categorization techniques to identify abusive language. Arguably the most well-known of these is the Google incubator [Jigsaw](https://jigsaw.google.com/), whose self-described mission is to "tackle some of the toughest global security challenges facing the world today—from thwarting online censorship to mitigating the threats from digital attacks to countering violent extremism to protecting people from online harassment." Their project [Perspective](https://www.perspectiveapi.com/) provides an API for querying a "toxicity" language model, one trained on data manually labeled on a spectrum from "very healthy" to "very toxic." They define "toxic" as "a rude, disrespectful, or unreasonable comment that is likely to make you leave a discussion." Much of their code [is freely available online, on GitHub](https://github.com/conversationai). Among these code repositories are moderator support programs, that use the Perspective API and automatically label contributions for toxicity on comment platforms such as [Reddit](https://github.com/conversationai/conversationai-moderator-reddit), [WordPress](https://github.com/conversationai/conversationai-moderator-wordpress), and [Discourse](https://github.com/conversationai/conversationai-moderator-discourse).
[Those that have tested the API, however, report mixed results,](https://qz.com/918640/alphabets-hate-fighting-ai-doesnt-understand-hate-yet/) a fact which has led one commentator to posit that the AI ["mistakes civility for decency"](https://motherboard.vice.com/en_us/article/qvvv3p/googles-anti-bullying-ai-mistakes-civility-for-decency). 

The related Wikimedia Foundation project [Detox](https://meta.wikimedia.org/wiki/Research:Detox) publishes previously mentioned public data sets, and also provides an API for querying two of their models: of personal attacks and aggressive tone [@wulczyn_ex_2017]. Our tests of the API, like those of Perspective above, show problems handling ambiguity: the threatening phrase "be careful, you might find some white powder in an envelope come in the mail one day" was rated as 1% aggressive, while the more playfully celebratory "I'm going to nominate you for the Nobel prize, you brilliant man" was rated 61% aggressive.

The Mozilla-funded [Coral Project](https://coralproject.net/) publishes a comment moderation assistance tool called [Talk](https://blog.coralproject.net/talk-features/), in use by the New York Times, in which language containing banned words is rejected, and suspect language is flagged or highlighted to aid moderators. It is unclear whether the project uses machine learning techniques or simply a dictionary-based approach, however.  

More proprietary systems also exist. A number of US patents---US5796948, US8868408, US8473443, US7818764, and US20080109214---describe "offensive message interceptors," "methods for word offensiveness processing," "inappropriate content detection," "methods for monitoring blocked content," and "methods for computerized psychological content analysis," respectively.

## Computational Detection of Abusive Language, Behaviors, or People (Jonathan)

### General Classification Studies

 - Tokenization and pre-processing
 - Features used
 - Classifiers used
 - Meta-classification

### Detection of Quality, Formality

 - Formality scores
 - Grammatical quality
 
### Detection and Analysis of Swearing

 - Disambiguation of friendly and abusive swearing

### Sentiment Analysis

### Metadata Analysis

 - Detection of bots, trolls
 - Social network theory
 - Trust propogation

### Related Fields

 - Detection of deceptive opinion spam 
 - Detection of misinformation

## And the Law (Colin)
### US Federal Law
- section 230 of the Communications Decency Act
- Civil Rights Law
-[Elonis v United States](https://www.supremecourt.gov/opinions/14pdf/13-983_7l48.pdf)
  - [menacing behavior online thrown out by supreme court](http://www.pewresearch.org/fact-tank/2015/06/01/the-darkest-side-of-online-harassment-menacing-behavior/) 
### European Law
- [Code of Conduct on Countering Illegal Hate Speech Online](ec.europa.eu/justice/fundamental-rights/files/hate_speech_code_of_conduct_en.pdf)
- [Code of Conduct signed by Facebook, Microsoft, Twitter and YouTube](https://www.theguardian.com/technology/2016/may/31/facebook-youtube-twitter-microsoft-eu-hate-speech-code)
### International Law
- International Covenant on Civil and Political Rights (ICCPR)

## Counterspeech (Colin)

Counter Speech can address the perpetrator of hate speech or the cyber bystanders whose discourse norms are shaped by seeing hate speech go uncontested. For hateful speakers with deeply ingrained hate, counter speech is less effective, but it can still alter their discourse to be less overtly hateful which in itself can also positively affect the cyber bystanders.

The guiding principle of counterspeech is the liberal ideal that more speech is the best remedy to harmful speech. This is partially informed by the idea that if you delete hateful content from one platform that there is always somewhere else to go (I’m not sure this is totally convincing).

### Recommended Strategies
- Warning of consequences
  - Remind speaker of harm done by speech
  - Remind of offline consequences and the permanence of online communication
  - Remind of online consequences (blocking, reporting, suspended account)
  - Mainly effective at getting hate speech deleted and doesn't necessarily change speakers POV
- Shaming and Labeling
  - Labeling the speech (not speaker) as bigoted, misogynist, etc
  - Helpful to cyberbystanders
  - Speaker ‘may not have known’ so it is better not to make a personal attack and to only to label the speech has harmful
- Empathy and Affiliation
  - Change the tone to friendly, empathetic or peaceful
  - Affiliate with speaker and establish a connection (ex: I am also a conservative, but…)
  - Affiliate with targeted group (ex: what you said hurt me as an asian…)
  - Changing the tone is more effective when the speaker affiliates with counterspeaker
- Humor
  - Neutralize hateful speech that is seen as dangerous and intimidating
  - Attract larger audience to the counterspeech’s message
  - Use humor to soften the message of counterspeech that could otherwise come off as hostile or aggressive (ex: [it’s time to stop posting cat](http://images.wikia.com/glee/images/c/c7/Imgur_gallery_hZSfl_It%27s_Time_to_Stop_Posting.jpg))
- Images
  - Can make counterspeech more viral
  - Counterspeech is generally more effective when it is emotive rather than rational/logical so images can be a good way to “send people along emotive pathways”

### Discouraged Strategies
- Hostile or aggressive tone and insults
  - Can cause backfire or speaker to dig in their heels
- Fact-checking
  - Fact checking may sway cyber bystanders, but it is unlikely to influence original speaker
  - Speaker will find a way to fit the new facts presented to the conclusions they are already committed to
    - Social psychologists call this the backfire effect where challenging someone’s views with facts leads them to hold those views even more firmly
  - Pointing out hypocrisy can be good for bystanders but usually not for speaker 
- Harassment and Silencing

### Successful counterspeech is indicated by: 
- Speaker shifts their discourse if not also their beliefs 
- Speaker apologizes, recants, or deletes original hate speech
- Discourse norms of the cyberbystanders are positively affected
- Hate speech narratives delegitimized (even if speaker is not swayed)
- More counter speech is elicited from the audience (this is good until it turns into harassment/dogpiling) 

### Types

A useful method of typologizing harmful speech online is by distinguishing between the types of exchanges (vectors). (This is based on the models put forward in Counterspeech on Twitter: A Field Study)
 
 - One-to-one: one person deploying counterspeech against one person’s hate speech
 - One-to-many: one person deploying counterspeech against many people’s hate speech
 - Many-to-one: many people deploying counterspeech against one person’s hate speech
 - Many-to-many: many people deploying counterspeech against many people’s hate speech
 
Source: 
- [Counterspeech on Twitter: A Field Study, by Susan Benesch, Derek Ruths, Kelly Dillon, Haji Mohammad Saleem, and Lucas Wright](https://dangerousspeech.org/counterspeech-on-twitter-a-field-study/)
- [Considerations for Successful Counterspeech,  by Susan Benesch, Derek Ruths, Kelly Dillon, Haji Mohammad Saleem, and Lucas Wright](https://dangerousspeech.org/considerations-for-successful-counterspeech/)

 
 ###  Kevin Munger's Study: [Tweetment Effects on the Tweeted: Experimentally Reducing Racist Harassment](https://link.springer.com/article/10.1007/s11109-016-9373-5)
 
 Kevin Munger conducted a study where he identified 231 twitter accounts operated by white men that regularly used racial slurs against black twitter accounts. Munger created a variety of twitter ‘bots’ with different amounts of followers, and some with a white male avatar and others with a black male avatar. The bots would send the following tweet in response to  detected racist tweets by the identified twitter accounts: "@[subject] Hey man, just remember that there are real people who are hurt when you harass them with that kind of language.”

Munger found that accounts confronted by the white male twitter bot with a lot of followers were most likely to alter their language in future posts. Around 27% of users stopped using the n-word in their posts the following weeks. Munger’s findings corroborate existing theories on counterspeech that tell us that counterspeech is most effective when done by someone with whom the harmful speaker identifies (in this case white males identify with other white men). On the other hand, Munger’s findings cut against existing theories about how anonymity affects online behavior; the study found that accounts with personal information (name, photo, location, etc.) that used the n-word were significantly less likely to change their behavior after being rebuked by the bot, and in some cases would increase their usage. It was primarily accounts with little or no personal information that would redress their behavior. This goes against the theory that anonymity contributes to online hate speech, since those that were most anonymous were the most likely to alter their behavior. 

 
## Inoculation 
 Inoculation is a long-term method for fighting against hate speech that takes some time. It involves instilling values in a society that oppose hate speech, and deals especially with building the social-psychological tools necessary so that groups of people don't fall victim to the pressures of engaging in hate speech or being incited by it. 
 - An example of a group that deals with Inoculation is Radio la Benevolencija (RLB) a dutch nonprofit that produces entertainment for countries in central africa that deals with the psychology underlying incitement to hate and violence.
 - [Citron and Norton](http://web.a.ebscohost.com.ezproxy.cul.columbia.edu/ehost/pdfviewer/pdfviewer?vid=1&sid=1c840371-b7ff-4ba7-a74e-8848b5bae30a%40sessionmgr4008) suggest that internet intermediaries and society at large  (especially public schools) play a stronger role in fostering digital citizenship   

## Calling out & doxxing
 - Gettign racists fired
 - Yes you’re racist/sexist
 - Goodbye felipe


 
# Future Directions

## Potential Applications of Related Fields (Jonathan)

### Psycholinguistics

 - Linguistic properties of emotional speech
 - "Language of psychopathy"

### Fusions of Existing Approaches 

 - Formality detection and social network theory
 - Quality ranking

## Automated Counterspeech 
