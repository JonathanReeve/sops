---
title: A Safer Online Public Square
authors: Jonathan Reeve, Colin Muller
---

# Problems

## Introdution 

Perhaps the most challenging step in developing social or technological tools for promoting a ‘safer online public square’ is defining what sort of speech constitutes a threat to the civility and safety of members of online communities.

Where do we draw the line between off-color and offensive content and content that inflicts harm or hate upon an individual or group? Additionally, how can a third party moderator (or algorithm) gauge the context of the speech as well as the the subjective perception of the speech by both speaker and target of the speech? 

Some critics of attempts to filter out harmful speech online claim that it violates first amendment free speech principles. These arguments vary, but generally point to the danger that   content moderation may silence dissenting voices or unpopular opinions. On the other hand, defenders of content moderation assert that  social media platforms have the right to remove content at their own discretion as they are not government agents who are held to first amendment standards. First amendment legal scholars have split the debate of free speech and content intermediaries between [the right to speak and the right to hear/be heard](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1205674) , with some arguing that the mere right to speak is insufficient if the media has the power to suppress any platform that this speech may have. Conversely, an individuals ability (or right?) to rapidly draft and publish a hateful tweet does necessitate that those targeted by the speech must view the harmful post.
 - Other concerns about moderation include threats to anonymity and privacy, no platforming, [silencing marginalized voices](https://www.propublica.org/article/facebook-hate-speech-censorship-internal-documents-algorithms), [amplifying hate speech by calling it out](http://archives.cjr.org/minority_reports/reprint_reporting_and_race.php), and fixing virtual identites to legal identities through ['real name' policies](https://www.theguardian.com/world/2017/jun/29/facebook-real-name-trans-drag-queen-dottie-lux)

There are many categories of speech that fall under the umbrella of harmful speech (The main categories are harassment, bullying, hate speech, and dangerous speech), but the boundaries between these different categories are neither rigid nor clear. The ambiguity posed by these various categories has led some to abandon formal definitions and seek different approaches to classifying harmful speech (see section below). Beyond this issue of classifying speech, there remains the problem of what one subjectively perceives as harmful speech; one [study](https://pdfs.semanticscholar.org/db55/11e90b2f4d650067ebf934294617eff81eca.pdf) found only 33% overall agreement between students of different races asked to assess the degree to which tweets were racially offensive.

### Statement of purpose of our preliminary report (to discuss at meeting)


## Taxonomies

There is significant corpus of writing by scholars attempting to formulate a definition and taxonomy of harmful speech online. These classifications will often vary based on the purpose of the definition which varies from academic research, legal recommendations, or advocacy. These different purposes result in varying breadth and scope of definitions, and disagreement over definitions is one of the main challenges in compiling data from different sources on the frequency of harmful speech online (as the specific behavior being monitored in different studies varies widely). Despite these challenges, it is useful to outline the different categories of harmful online behavior, as these different types manifest themselves differently and will require different approaches for intervention.

 - Hate speech
   - “An expression that denigrates or stigmatizes a person or people based on their membership of a group that is usually but not always immutable, such as an ethnic or religious group. Sometimes other groups, defined by disability or sexual orientation, for example, are included.” Source: Benesch, Susan. Defining and diminishing hate speech. 2014. 

 - Bullying:
   - “Cyberbullying is any behavior performed through electronic or digital media by individuals or groups that repeatedly communicates hostile or aggressive messages intended to inflict harm or discomfort on others. Additionally, the following addendum may be included with the definition.... In cyberbullying experiences, the identity of the bully may or may not be known. Cyberbullying can occur through electronically mediated communication at school; however, cyberbullying behaviors commonly occur outside of school as well." Tokunaga, Robert S. "Following you home from school: A critical review and synthesis of research on cyberbullying victimization." Computers in human behavior 26.3 (2010): 278.
   - Definitions of cyberbullying are derived from definitions of traditional bullying which generally include the 3 conditions of 1. Repeated behavior 2. Psychological torment 3. Carried out with intent.
     - the question of the repetition of bullying behavior is complicated online since a single post on a social media site can have a repeated effect on its target with every ‘like’ or ‘share’ 
   - Cyberbullying is seen more as an “opportunistic offense” since individuals who don’t engage in traditional bullying will do so online since of the low threat of being caught and the lack of any clear disciplinary authority

 - Harassment and stalking
 - Dangerous speech
   - “speech that can inspire or catalyze intergroup violence” Source: [Counterspeech on Twitter: A Field Study](https://dangerousspeech.org/counterspeech-on-twitter-a-field-study/) 
   - Susan Benesch puts forward the following conditions that make the likelihood of speech resulting in group violence : 
     - there is a “powerful speaker with a high degree of influence;”
     - there is a receptive audience with “grievances and fear that the speaker can cultivate;”
     - a speech act “that is clearly under-stood as a call to violence;”
     - a social or historical context that is “propitious for violence, for any of a variety of reasons;”
     - An “influential means of dissemination.”” 
     - Source: http://www.worldpolicy.org/sites/default/files/Dangerous%20Speech%20Guidelines%20Benesch%20January%202012.pdf 

 - Abuse of journalists
   - "Gamergate"
 - Related language patterns that, while not strictly abusive language, could be useful to its detection
   - Misinformation (e.g. Russian fake news)
   - Deceptive opinion spam (e.g. fake Amazon product reviews) 
   
   ### Alternative approaches to classifying harfmul speech:

Besides the approach of defining and taxonomizing different forms of hate speech, some have formulated other vectors for recognizing harmful speech online. 

 #### Implicit v. Explicit & Generalized v. Directed Abusive Language 
   - Waseem et al. propose two primary factors for typologizing abusive language rather than attempting to define various terms such as abusive language, hate speech, cyberbullying, cyber harassment. They propose the following two factors: 
     - Is the language directed towards a specific individual or entity or is it directed towards a generalized group?
     - Is the abusive content explicit or implicit?     
   - Typologizing based on these two factors is useful, as there can be a lot of overlap and ambiguity when relying on stricter defintions. One shortcoming of this typology is that by making the distinction between directed/generalized attacks, the research may downplay the fact that hate speech, even when verbally directed at an individual (ex: calling someone a racial slur) is a crime against an entire sub-group of people. 
   - Source: [Waseem, Zeerak, et al. "Understanding Abuse: A Typology of Abusive Language Detection Subtasks." arXiv preprint arXiv:1705.09899 (2017).](http://www.aclweb.org/anthology/W17-3012)
#### Justice Stewart's rule: “I know it when I see it”
Justice Stewart famously asserted that “I know it when I see it” when referring to identifying obscenity. It seems to be the consensus that this approach is not applicable to identifying hate speech due to the variety of forms of speech and contexts which one could identify as hate speech.

There are instances where specific epithets or insults are used and an outsider or scholar may see hate speech but the speaker/recipient do not. Henry Louis Gates Jr. for this reason asserted that we should not “spend more time worrying about speech codes than coded speech.” Since a lot of hate speech can be coded or masked by symbols.

The discussion of Stewart’s “I know it when I see it” points to a central difficulty in defining hate speech since it requires assessing the subjectivity and intention of both the perpetrator and the victim. However, only some definitions include the component of intention on the part of the perpetrator, and definitions also vary on how they define harm to the victim.


## Underlying Social-Psychological Causes of Harmful online behavior

While theory building on the underlying causes of harmful speech online is generally underdeveloped and often not rigorously proven with empirical research, ([Tokunaga 2010](http://www.sciencedirect.com/science/article/pii/S074756320900185X)), existing social-psychological theories helps us better determine effective forms of intervention. This vein of research is particularly useful in emphasizing the impact that an individual’s harmful speech can have on their social group, since “cyberbystanders” witnessing of abusive language online impacts their  understanding of acceptable online norms. This research then reminds us that when intervening in the name of a safer online public sphere, it is not only the speaker and recipient that we must pay attention to, but also those bystanders and digital onlookers who may also happen to witness the encounter. 

### Anonymity: online disinhibition, deindividuation, and depersonalization

-Suler's theory of the online disinhibtion effect
- Barlett and Gentile Model
- deindividuation: loss of sence of self
- depersonalization: prioritizing one's identity with the group over one's individualized identity
### Mimicry Effect
 - This theory holds that boosting positive content rather than deleting harmful content will foster an environment where other members of the website will contribute positive content
 
 ### Backfire effect
 - Political psychologists Nyhan and Reifler have researched the “backfire effect” where attempts at correcting misperceptions or misinformed beliefs results in firmer beliefs in the misperception or misinformation
   - “individuals who receive unwelcome information may not simply resist challenges to their views. Instead they may come to support their original opinion even more strongly”  Nyhan, Brendan, and Jason Reifler. "When corrections fail: The persistence of political misperceptions." Political Behavior 32.2 (2010): 307.
   - It is important to consider the backfire effect in looking at effective forms of intervention/counterspeech, since often presenting facts or engaging in logical/reasoned debate is not the most effective strategy
   - A similar psychological phenomenon is “motivated reasoning” where people make a strong effort to support the conclusions they seek despite being exposed to contradictory facts
   
   ### Temporal clustering of hate speech
  - prejudicial crimes are strongly influenced in the short term due to publicized events such as murders committed by a minority, this amplification in hate speech usually lasts about 2 weeks.  King, R.D., and G.M. Sutton. 2013. “High Times for Hate Crime: Explaining the Temporal Clustering of Hate Motivated Offending.” Criminology 51 (4): 871–94
  - Hashtags can be a good indicator of temporal clustering: ex: #killallmuslims #ferguson #charliehebdo #brussels #banislam #baltimore #mizzou
## Statistics 

 - General stats related to abusive langauge
 - Cyberbulling stats
 - Hate speech stats

# Existing Approaches to Intervention
## Organizations 
### Advocacy Groups (Colin and Jonathan)

 - [Women's Media Center Speech Project](http://wmcspeechproject.com/)
 - Working to Halt Online Abuse
 - No Hate Speech Movement
 - Etc...

#### Legal Aid (Colin)
#### Education (Colin)

### Initiatives by Social Media Platforms

 - Official policies of Facebook, Twitter
 - Twitter's "progress on addressing online abuse"
 - Implementations of Perspective API on Facebook, Reddit
 - New social media outlets (Mastodon) created with these problems in mind
 - Moderation, flagging

### Databases and Datasets

 - Trolldor: "the global blacklist of twitter trolls"
 - Hatebase
 - Gamergate tweets
 - Public training data from Kaggle contest and other similar projects

### Organizations and Projects Employing Machine Learning 

 - Jigsaw (Google)
   - Perspective API
 - Wikimedia Foundation WikiDeTox project
 - Brief survey of US patents for detection of "offensiveness"
 
## Computational Detection of Abusive Language, Behaviors, or People (Jonathan)

### General Classification Studies

 - Tokenization and pre-processing
 - Features used
 - Classifiers used
 - Meta-classification

### Detection of Quality, Formality

 - Formality scores
 - Grammatical quality
 
### Detection and Analysis of Swearing

 - Disambiguation of friendly and abusive swearing

### Sentiment Analysis

### Metadata Analysis

 - Detection of bots, trolls
 - Social network theory
 - Trust propogation

### Related Fields

 - Detection of deceptive opinion spam 
 - Detection of misinformation

## And the Law (Colin)
### US Federal Law
- section 230 of the Communications Decency Act
- Civil Rights Law
-[Elonis v United States](https://www.supremecourt.gov/opinions/14pdf/13-983_7l48.pdf)
  - [menacing behavior online thrown out by supreme court](http://www.pewresearch.org/fact-tank/2015/06/01/the-darkest-side-of-online-harassment-menacing-behavior/) 
### European Law
- [Code of Conduct on Countering Illegal Hate Speech Online](ec.europa.eu/justice/fundamental-rights/files/hate_speech_code_of_conduct_en.pdf)
- [Code of Conduct signed by Facebook, Microsoft, Twitter and YouTube](https://www.theguardian.com/technology/2016/may/31/facebook-youtube-twitter-microsoft-eu-hate-speech-code)
### International Law
- International Covenant on Civil and Political Rights (ICCPR)

## Counterspeech (Colin)

 - Kevin Munger article

### Types

A useful method of typologizing harmful speech online is by distinguishing between the types of exchanges (vectors). (This is based on the models put forward in Counterspeech on Twitter: A Field Study)
 
 - One-to-one: one person deploying counterspeech against one person’s hate speech
 - One-to-many: one person deploying counterspeech against many people’s hate speech
 - Many-to-one: many people deploying counterspeech against one person’s hate speech
 - Many-to-many: many people deploying counterspeech against many people’s hate speech
 
## Inoculation 
 Inoculation is a long-term method for fighting against hate speech that takes some time. It involves instilling values in a society that oppose hate speech, and deals especially with building the social-psychological tools necessary so that groups of people don't fall victim to the pressures of engaging in hate speech or being incited by it. 
 - An example of a group that deals with Inoculation is Radio la Benevolencija (RLB) a dutch nonprofit that produces entertainment for countries in central africa that deals with the psychology underlying incitement to hate and violence.
 - [Citron and Norton](http://web.a.ebscohost.com.ezproxy.cul.columbia.edu/ehost/pdfviewer/pdfviewer?vid=1&sid=1c840371-b7ff-4ba7-a74e-8848b5bae30a%40sessionmgr4008) suggest that internet intermediaries and society at large  (especially public schools) play a stronger role in fostering digital citizenship   

 ## Calling out & doxxing
 - Gettign racists fired
 - Yes you’re racist/sexist
 - Goodbye felipe


 
# Future Directions

## Potential Applications of Related Fields (Jonathan)

### Psycholinguistics

 - Linguistic properties of emotional speech
 - "Language of psychopathy"

### Fusions of Existing Approaches 

 - Formality detection and social network theory
 - Quality ranking

## Automated Counterspeech 
